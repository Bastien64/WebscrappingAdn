from flask import Flask, render_template
import requests
from bs4 import BeautifulSoup
import re
import time  # Ajoutez 
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


app = Flask(__name__)

def get_site_name(url):
    # Utilisation d'une expression régulière pour extraire le nom du site
    match = re.search(r'www\.(.*?)\.(com|fr)', url)
    if match:
        return match.group(1)
    else:
        return "Nom du site non trouvé"

import requests
from bs4 import BeautifulSoup
import re

def scrape_emails(urls):
    unique_results = {}
    for url in urls:
        try:
            response = requests.get(url, verify=False, timeout=2)
            print("HTTP Response for", url, ":", response.status_code)
            soup = BeautifulSoup(response.text, 'html.parser')
            emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', response.text)
            unique_emails = list(set(emails))
            filtered_emails = [email for email in unique_emails if not (email.endswith('.png') or email.endswith('.webp'))]
            print("Emails trouvées pour", url, ":", filtered_emails)
            site_name = get_site_name(url)
            if site_name not in unique_results:
                unique_results[site_name] = {
                    "site_name": site_name,
                    "site_url": url,
                    "email": ", ".join(filtered_emails) if filtered_emails else "Aucune adresse e-mail trouvée"
                }
        except requests.exceptions.RequestException as e:
            print("Une erreur s'est produite lors de la connexion à", url, ":", e)

    return list(unique_results.values())




def scrape_Qwant_search_results(query):
    search_results = []
    print (search_results)
    driver = webdriver.Chrome()  # Assurez-vous d'avoir ChromeDriver installé et configuré sur votre système
    # Ouvrir la page Qwant
    driver.get(f"https://www.qwant.com/?l=fr&q={query}") 
    try:
        # Attendre que le bouton "Plus de résultats" soit présent et cliquable
        button = driver.find_element(By.XPATH, "//button[contains(text(),'Plus de résultats')]")
        # Cliquer sur le bouton
        button.click()
        time.sleep(5)
        button.click()
        # Pause pour laisser le temps aux résultats supplémentaires de se charger
        time.sleep(30)  # Vous pouvez ajuster cette valeur selon vos besoins
        # Récupérer le contenu de la page après avoir cliqué sur le bouton
        page_content = driver.page_source
        print (page_content)
        soup = BeautifulSoup(page_content, 'html.parser')
        # Chercher les liens de résultats de recherche avec la classe "result__link"
        search_result_links = soup.find_all('a', class_='external')
        for link in search_result_links[:500]:  # Limiter à 50 résultats
            url = link['href']  # Extraire l'URL du lien
            search_results.append(url)

    finally:
        # Fermer le pilote Selenium
        driver.quit()
    return search_results

@app.route('/')
def index():
    # Liste d'URLs à scraper (remplacée par les résultats de recherche Google)
    query = "createur site web tarbes"
    search_results = scrape_Qwant_search_results(query)
    all_results = scrape_emails(search_results)
    print("Tous les résultats trouvés:", all_results)
    return render_template('index.html', results=all_results)

if __name__ == '__main__':
    app.run(debug=True)